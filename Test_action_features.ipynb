{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.7 (default, May  6 2020, 04:59:01) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)] \n",
      "\n",
      "PyTorch 1.5.0 \n",
      "\n",
      "Torch-vision 0.6.0 \n",
      "\n",
      "Available devices:\n",
      "CPUs only, no GPUs found\n"
     ]
    }
   ],
   "source": [
    "# Regular Python libraries\n",
    "import sys\n",
    "from collections import deque #\n",
    "import io\n",
    "import requests\n",
    "import os\n",
    "from time import sleep, time\n",
    "from threading import Thread\n",
    "from IPython.display import Video\n",
    "\n",
    "# Third party tools\n",
    "import decord #\n",
    "import IPython.display #\n",
    "from ipywebrtc import CameraStream, ImageRecorder\n",
    "from ipywidgets import HBox, HTML, Layout, VBox, Widget, Label\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "# utils_cv\n",
    "sys.path.append(\"../../\")\n",
    "from utils_cv.action_recognition.data import KINETICS, Urls\n",
    "from utils_cv.action_recognition.dataset import get_transforms\n",
    "from utils_cv.action_recognition.model import VideoLearner\n",
    "from utils_cv.action_recognition.references import transforms_video as transforms\n",
    "from utils_cv.common.gpu import system_info, torch_device\n",
    "from utils_cv.common.data import data_path\n",
    "\n",
    "system_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FRAMES = 32  # 8 or 32.\n",
    "IM_SCALE = 128  # resize then crop\n",
    "INPUT_SIZE = 112  # input clip size: 3 x NUM_FRAMES x 112 x 112\n",
    "\n",
    "\n",
    "# prediction score threshold\n",
    "SCORE_THRESHOLD = 0.01\n",
    "\n",
    "# Averaging 5 latest clips to make video-level prediction (or smoothing)\n",
    "AVERAGING_SIZE = 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading r2plus1d_34_32_kinetics model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/jojo/.cache/torch/hub/moabitcoin_ig65m-pytorch_master\n",
      "Downloading: \"https://github.com/moabitcoin/ig65m-pytorch/releases/download/v1.0.0/r2plus1d_34_clip32_ft_kinetics_from_ig65m-ade133f1.pth\" to /Users/jojo/.cache/torch/hub/checkpoints/r2plus1d_34_clip32_ft_kinetics_from_ig65m-ade133f1.pth\n"
     ]
    }
   ],
   "source": [
    "learner = VideoLearner(\n",
    "    base_model=\"kinetics\",\n",
    "    sample_length=NUM_FRAMES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abseiling',\n",
       " 'air drumming',\n",
       " 'answering questions',\n",
       " 'applauding',\n",
       " 'applying cream',\n",
       " 'archery',\n",
       " 'arm wrestling',\n",
       " 'arranging flowers',\n",
       " 'assembling computer',\n",
       " 'auctioning']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS = KINETICS.class_names\n",
    "LABELS[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
